{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UJIIndoorLoc Response EDA and Classification - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous notebook](#UJIIndoorLoc-preprocess.ipynb), we performed various transformations on the independent variables of the raw UJIIndoorLoc dataset to prepare it for the machine learning. \n",
    "\n",
    "In this notebook, first, I focus on our response variables including building ID, floor ID, latitiude and longitude. Understanding the class imbalance in classification responses buildingID and floorID is important for training our machine learning models. Similarly, I analyze the distributions of our regression response variables latitude and longitude and their relationship with the building ID and floor ID. Second, I formulate the localization problem for the machine learning. Finally, I begin constructing machine learning framework first by focusing on regression without Floor and Building information. In future notebooks, I will model and evaluate cascade machine learning frameworks that perform building and floor classification before applying building and floor-optimized regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Response EDA](#response-eda)\n",
    "\n",
    "3. [Problem Formulation](#problem-formulation)\n",
    "\n",
    "4. [Multi-Variable Multivariate Regression](#regression)\n",
    "                \n",
    "6. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"setup\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Collection and Transformations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "\n",
    "# Statistical Testing\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Class imbalance \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Plotting \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['figure.figsize'] = [10,8]\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the transformed data from our previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17874, 150), (17874, 6), (1987, 150), (1987, 6))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca_crossval = pd.read_csv(\"data/X_pca_crossval.csv\",index_col=0)\n",
    "y_crossval = pd.read_csv(\"data/y_crossval.csv\",index_col=0)\n",
    "\n",
    "X_pca_holdout = pd.read_csv(\"data/X_pca_holdout.csv\",index_col=0)\n",
    "y_holdout = pd.read_csv(\"data/y_holdout.csv\",index_col=0)\n",
    "\n",
    "X_raw_crossval = pd.read_csv(\"data/X_raw_crossval.csv\",index_col=0)\n",
    "X_raw_holdout = pd.read_csv(\"data/X_raw_holdout.csv\",index_col=0)\n",
    "\n",
    "X_pca_crossval.shape,y_crossval.shape,X_pca_holdout.shape,y_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_raw_crossval.fillna(value=100,inplace=True)\n",
    "X_raw_holdout.fillna(value=100,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"building-eda\"></a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_crossval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few sections, we explore the characteristics of the different response variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"response-eda\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Response EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_crossval[['BUILDINGID']],kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "\n",
    "1. In our training samples, building 2 has the clear majority with it's count being slightly lower than the sum of building 0 and building 1.\n",
    "\n",
    "2. Building 0 and building 1 have roughly the same representation in the training data.\n",
    "\n",
    "Clearly, there is an imbalance among the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markers = ('s', 'x', 'o', '^', 'v')\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y_crossval['BUILDINGID']))])\n",
    "\n",
    "for idx, cl in enumerate(np.unique(y_crossval['BUILDINGID'])):\n",
    "        plt.scatter(x=y_crossval.loc[y_crossval.BUILDINGID== cl]['LATITUDE'], \n",
    "                    y=y_crossval.loc[y_crossval.BUILDINGID== cl]['LONGITUDE'],\n",
    "                    alpha=0.6, \n",
    "                    c=cmap(idx),\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)\n",
    "\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot illustrates the locations of the buildings in the campus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markers = ('s', 'x', 'o', '^', 'v')\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y_crossval['BUILDINGID']))])\n",
    "\n",
    "for idx, cl in enumerate(np.unique(y_crossval['BUILDINGID'])):\n",
    "        plt.scatter(x=X_pca_crossval.loc[y_crossval.BUILDINGID== cl].iloc[:,0], \n",
    "                    y=X_pca_crossval.loc[y_crossval.BUILDINGID== cl].iloc[:,1],\n",
    "                    alpha=0.6, \n",
    "                    c=cmap(idx),\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot illustrates how the buildingID are distributed across the top two PCA dimensions. Later, I explore the machine learning approaches for the building classification.\n",
    "\n",
    "Remember PCA is an *unsupervised learning* technique for dimensionality reduction. So, it is quite possible the two top PCA components might not have explained our response variable well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name = \"floor-eda\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Floor EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"FLOOR\", hue=\"BUILDINGID\", data=y_crossval,orient=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "    \n",
    "1. Buildings 0 and 1 have 4 floors whereas Building 2 has 5 floors.\n",
    "\n",
    "2. Expectedly, the samples from Building 2 are consistently the highest across all the floors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"Problem Formulation\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a name=\"problem-formulation\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Error Metric\n",
    "\n",
    "The overall goal of this project is to build models for accurate indoor localization. The mean positioning error expressed as the mean Euclidean distance between the real and estimated locations. **However, in multi-building, multi-floor environments as in our problem, just the positioning error due to Euclidean distance is not enough.** Wrong floor and wrong building classification are not desirable as the actual movement from the predicted location to the actual location might involve great displacement.\n",
    "\n",
    "Therefore, we include penalty terms to the mean error equation to penalize failures in floor and building classification. This was introduced in the [2015 EvAAL-ETRI competition](http://content.iospress.com/articles/journal-of-ambient-intelligence-and-smart-environments/ais421). The cost function can be expressed as follows:\n",
    "\n",
    "$positioning\\_error(actual,predicted)= euclidean\\_distance(actual,predicted) + penalty_{floor}*fail_{floor} + penalty_{building}*fail_{building}$\n",
    "\n",
    "where $fail_{floor}$ and $fail_{building}$ indicate if the floor and building are incorrectly identified, $penalty_{floor}$ and $penalty_{building}$ are the penalty values applied for wrongful classification of floor and building respectively. The penalty values were set to 4 and 50 respectively in the third track of the competition ([Source](http://ieeexplore.ieee.org/document/7743679/)). Expectedly, the penalty for building classification failure is higher than that of floor classification failure. In this project, I utilize the same penalty term values for the error metric.\n",
    "\n",
    "### 3.2 Machine Learning Methodology\n",
    "\n",
    "Because of the added penalty terms, we cannot simply perform regression for the Latitude and Longitude. Separate models might have to be trained per-floor and per-building. Hence, the building and floor need to be classified first.\n",
    "\n",
    "However, I first analyze the regression variables in isolation without incorporating the buildingID and FloorID. The framework built will be used for comparison against the Cascade framework that incorporates the building and floor information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <a name = \"regression\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Variable Multivariate Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The key concepts of building the regression framework include:**\n",
    "\n",
    "1. *MultiOutputRegressor*: We have the response as a vector of 2-dimensions (Latitude and Longitude). Not every regression method in scikit-learn can handle this sort of problem. Most linear models provide this capability but for those that don't, a new class MultiOutputRegressor is available for parallelization of regressors for multivariate output.\n",
    "\n",
    "2. *Linear Regression Models*: First, I will focus on Linear regression and its variants including Lasso, Kernel Ridge.\n",
    "\n",
    "3. *Polynomial Features*: Consider Polynomial Features including quadratic and cubic for addressing non-linearities.\n",
    "\n",
    "4. *Other Regression Models*: ExtraTreesRegressor, RandomForestRegressor, XGBoostRegressor\n",
    "\n",
    "5. *Stacking*: Simple Average, XGBoost stacking as shown in this [Kaggle kernel](https://www.kaggle.com/eliotbarr/house-prices-advanced-regression-techniques/stacking-starter/run/598752)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17874, 150), (17874, 2), (1987, 150), (1987, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_pca_crossval)\n",
    "y_train = y_crossval[['LATITUDE','LONGITUDE']]\n",
    "\n",
    "X_test = np.array(X_pca_holdout)\n",
    "y_test = y_holdout[['LATITUDE','LONGITUDE']]\n",
    "\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store nested cross-validation scores\n",
    "model_scores = {}\n",
    "\n",
    "# Dictionary to store model and param grid mapping\n",
    "model_param_grid = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few sub-sections, we perform nested cross-validation on the different model families. In nested cross-validation, the inner fold performs the parameter tuning and the outer fold is used for the validation performance.\n",
    "\n",
    "Before we begin the model assessment, let's write a function to simplify the nested cross-validation operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nested_crossval(reg_list,reg_labels, model_param_grid=model_param_grid, model_scores = model_scores,\n",
    "                    X = X_train, y= y_train, label_extension = None):\n",
    "    '''\n",
    "    Inputs:\n",
    "    reg_model        : List of Regression model instances\n",
    "    reg_label        : List of Regression model labels\n",
    "    model_param_grid : List of parameter grids\n",
    "    X                : explanatory variables \n",
    "    y                : response variable array\n",
    "    model_scores     : Dictionary to store nested cross-validation scores\n",
    "    label_extension  : Extension to regression label in model_scores key\n",
    "    \n",
    "    Outputs:\n",
    "    model_scores     : Updated dictionary of nested cross-validation scores\n",
    "    '''\n",
    "\n",
    "    \n",
    "    for reg_model, reg_label in zip(reg_list, reg_labels):\n",
    "    \n",
    "        #print(param_grid)\n",
    "    \n",
    "        gs = (GridSearchCV(estimator=reg_model, \n",
    "                            param_grid=model_param_grid[reg_label], \n",
    "                            cv=2,\n",
    "                            scoring = 'neg_mean_squared_error',\n",
    "                            n_jobs = 1))\n",
    "    \n",
    "        scores = cross_val_score(estimator=gs,\n",
    "                                 X=X,\n",
    "                                 y=y,\n",
    "                                 cv=5,\n",
    "                                 scoring='neg_mean_squared_error')\n",
    "        scores = np.sqrt(np.abs(scores))\n",
    "        \n",
    "        if label_extension:\n",
    "            reg_label += '_' + label_extension\n",
    "        \n",
    "        print(\"RMSE: %0.2f (+/- %0.2f) [%s]\"\n",
    "              % (scores.mean(), scores.std(), reg_label))\n",
    "        \n",
    "        model_scores[reg_label] = scores\n",
    "        \n",
    "    \n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Linear Models and Variants\n",
    "\n",
    "In this sub-section, I analyze the performance of Linear Regression models and its regularization variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 25.25 (+/- 0.41) [Ridge]\n",
      "RMSE: 25.26 (+/- 0.43) [Lasso]\n"
     ]
    }
   ],
   "source": [
    "## Linear Models\n",
    "\n",
    "# Ridge Regression\n",
    "pipe_ridge = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', Ridge(random_state=1))])\n",
    "\n",
    "# Lasso\n",
    "pipe_lasso = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', Lasso(random_state=1))])\n",
    "\n",
    "param_grid_lm= {\n",
    "    'reg__alpha':[0.01,0.1,1,10],\n",
    "}\n",
    "\n",
    "reg_lm = [pipe_ridge,pipe_lasso]\n",
    "reg_labels_lm = ['Ridge','Lasso']\n",
    "model_param_grid['Ridge'] = param_grid_lm\n",
    "model_param_grid['Lasso'] = param_grid_lm\n",
    "\n",
    "model_scores = nested_crossval(reg_lm,reg_labels_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso': array([ 25.49069375,  24.50169635,  25.78675344,  25.24330896,  25.29979238]),\n",
       " 'Ridge': array([ 25.43926941,  24.53596898,  25.780613  ,  25.20794527,  25.30487622])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, ridge regression and Lasso provide nearly the same performance but still very far from our baseline of 7.5m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Polynomial Regression\n",
    "\n",
    "In this sub-section, we analyze the non-linear variations of Regression by incorporating higher-order features into the Regression Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quadratic = PolynomialFeatures(degree=2)\n",
    "X_quad = quadratic.fit_transform(X_train)\n",
    "\n",
    "pipe_ridge_poly = Pipeline([('scl', StandardScaler()),\n",
    "                            ('pca', PCA(n_components=100)),\n",
    "                            ('reg', Ridge(random_state=1))])\n",
    "\n",
    "# Lasso\n",
    "pipe_lasso_poly = Pipeline([('scl', StandardScaler()),\n",
    "                            ('pca', PCA(n_components=100)),\n",
    "                            ('reg', Lasso(random_state=1))])\n",
    "\n",
    "reg_lm_quad = [pipe_ridge_poly,pipe_lasso_poly]\n",
    "reg_labels_lm_quad = ['Ridge_Quadratic','Lasso_Quadratic']\n",
    "model_param_grid['Ridge_Quadratic'] = param_grid_lm\n",
    "model_param_grid['Lasso_Quadratic'] = param_grid_lm\n",
    "\n",
    "model_scores = nested_crossval(reg_lm_quad,reg_labels_lm_quad, X= X_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's save our data into a file\n",
    "f = open(\"model_scores_lm.pckl\", \"wb\")\n",
    "pickle.dump(model_scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl_file = open('model_scores_lm.pckl', 'rb')\n",
    "\n",
    "model_scores = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lasso': array([ 25.49069375,  24.50169635,  25.78675344,  25.24330896,  25.29979238]),\n",
       " 'Ridge': array([ 25.43926941,  24.53596898,  25.780613  ,  25.20794527,  25.30487622])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cubic = PolynomialFeatures(degree=3)\n",
    "X_cubic = cubic.fit_transform(X_train)\n",
    "\n",
    "reg_lm_cube = [pipe_ridge_poly,pipe_lasso_poly]\n",
    "reg_labels_lm_cube = ['Ridge_Cubic','Lasso_Cubic']\n",
    "model_param_grid['Ridge_Cubic'] = param_grid_lm\n",
    "model_param_grid['Lasso_Cubic'] = param_grid_lm\n",
    "\n",
    "model_scores = nested_crossval(reg_lm_cube,reg_labels_lm_cube, X= X_cubic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 K Nearest Neighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.93 (+/- 0.26) [KNN]\n"
     ]
    }
   ],
   "source": [
    "pipe_knn = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', KNeighborsRegressor())])\n",
    "\n",
    "grid_param_knn = {\n",
    "    'reg__n_neighbors': [2,3,5,7],\n",
    "    'reg__weights': ['uniform','distance'],\n",
    "    'reg__metric': ['euclidean','minkowski','manhattan'],\n",
    "    'reg__n_jobs': [-1]\n",
    "}\n",
    "\n",
    "model_param_grid['KNN'] = grid_param_knn\n",
    "\n",
    "model_scores = nested_crossval([pipe_knn],['KNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's save our data into a file\n",
    "f = open(\"model_scores_knn.pckl\", \"wb\")\n",
    "pickle.dump(model_scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KNN': array([ 6.44366044,  5.9055819 ,  5.74672049,  5.7716296 ,  5.76268187]),\n",
       " 'Lasso': array([ 25.49069375,  24.50169635,  25.78675344,  25.24330896,  25.29979238]),\n",
       " 'Ridge': array([ 25.43926941,  24.53596898,  25.780613  ,  25.20794527,  25.30487622])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.78 (+/- 0.16) [Random Forests]\n",
      "RMSE: 8.89 (+/- 0.15) [Extra Trees]\n"
     ]
    }
   ],
   "source": [
    "# Random Forests\n",
    "reg_rf = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Extra Trees\n",
    "reg_et = ExtraTreesRegressor(random_state=1)\n",
    "\n",
    "param_grid_tree = {\n",
    "    'n_jobs': [-1],\n",
    "    'n_estimators': [10,30,50,70,100],\n",
    "    'max_features': [0.25,0.5,0.75],\n",
    "    'max_depth': [3,6,9,12],\n",
    "    'min_samples_leaf': [5,10,20,30]\n",
    "}\n",
    "\n",
    "reg_tree = [reg_rf,reg_et]\n",
    "reg_labels_tree = ['Random Forests','Extra Trees']\n",
    "model_param_grid['Random Forests'] = param_grid_tree\n",
    "model_param_grid['Extra Trees'] = param_grid_tree\n",
    "\n",
    "model_scores = nested_crossval(reg_tree,reg_labels_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, let's save our data into a file\n",
    "f = open(\"model_scores_tree.pckl\", \"wb\")\n",
    "pickle.dump(model_scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Extra Trees': array([ 9.17370814,  8.82194108,  8.75181561,  8.90668164,  8.80848569]),\n",
       " 'KNN': array([ 6.44366044,  5.9055819 ,  5.74672049,  5.7716296 ,  5.76268187]),\n",
       " 'Lasso': array([ 25.49069375,  24.50169635,  25.78675344,  25.24330896,  25.29979238]),\n",
       " 'Random Forests': array([ 7.07787616,  6.60653919,  6.72760227,  6.80752508,  6.69423105]),\n",
       " 'Ridge': array([ 25.43926941,  24.53596898,  25.780613  ,  25.20794527,  25.30487622])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Support Vector Regression\n",
    "pipe_svr = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', MultiOutputRegressor(SVR()))])\n",
    "\n",
    "grid_param_svr = {\n",
    "    'reg__estimator__C': [0.01,0.1,1,10],\n",
    "    'reg__estimator__epsilon': [0.1,0.2,0.3],\n",
    "    'reg__estimator__degree': [2,3,4]\n",
    "}\n",
    "\n",
    "model_param_map[pipe_svr] = grid_param_svr\n",
    "\n",
    "# Multi-Layer Perceptron\n",
    "pipe_mlp = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', MLPRegressor(random_state=1))])\n",
    "\n",
    "grid_param_mlp = {\n",
    "    'alpha': [0.0001,0.001,0.01,0.1],\n",
    "    'learning_rate': ['constant','invscaling','adaptive']\n",
    "}\n",
    "\n",
    "model_param_map[pipe_mlp] = grid_param_mlp\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for reg_model, reg_label in zip(reg_list, reg_labels):\n",
    "    \n",
    "    gs = (GridSearchCV(estimator=reg_model, \n",
    "                      param_grid=model_param_map[reg_model], \n",
    "                      cv=2,\n",
    "                      scoring = 'neg_mean_squared_error',\n",
    "                      n_jobs = -1))\n",
    "    \n",
    "    scores = cross_val_score(estimator=gs,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=5,\n",
    "                             scoring='neg_mean_squared_error')\n",
    "    print(\"Neg Mean Square Error: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), reg_label))\n",
    "    \n",
    "    model_scores[reg_label] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, Extra Trees Regressor performs the best in the nested cross-validation slightly better than Random Forests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation, metrics\n",
    "\n",
    "def modelfit(alg, X_train, y_train,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train,eval_metric='rmse')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X_train)\n",
    "    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % metrics.mean_squared_error(y_train, dtrain_predictions))\n",
    "    #print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
    "    \n",
    "xgb_single = XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=100,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_multi = MultiOutputRegressor(SVR(),n_jobs=-1)\n",
    "\n",
    "#Fit the algorithm on the data\n",
    "est_multi.fit(X_train, y_train)\n",
    "y_multi = est_multi.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "s = 50\n",
    "a = 0.4\n",
    "plt.scatter(y_test['LATITUDE'], y_test['LONGITUDE'],\n",
    "            c=\"navy\", s=s, marker=\"s\", alpha=a, label=\"Data\")\n",
    "plt.scatter(y_multi[:, 0], y_multi[:, 1],\n",
    "            c=\"cornflowerblue\", s=s, alpha=a,\n",
    "            label=\"Multi Estimator score=%.2f\" % est_multi.score(X_test, y_test))\n",
    "#plt.xlim([-6, 6])\n",
    "#plt.ylim([-6, 6])\n",
    "plt.xlabel(\"LATITUDE\")\n",
    "plt.ylabel(\"LONGITUDE\")\n",
    "plt.title(\"Comparing real and the multi-output meta estimator\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_svr = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', MultiOutputRegressor(SVR()))])\n",
    "\n",
    "grid_param_svr = {\n",
    "    'reg__estimator__C': [0.1,1,10]\n",
    "}\n",
    "\n",
    "gs_svr = (GridSearchCV(estimator=pipe_svr, \n",
    "                      param_grid=grid_param_svr, \n",
    "                      cv=2,\n",
    "                      scoring = 'neg_mean_squared_error',\n",
    "                      n_jobs = -1))\n",
    "\n",
    "gs_svr = gs_svr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_svr.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_knn = Pipeline([('scl', StandardScaler()),\n",
    "            ('reg', KNeighborsRegressor())])\n",
    "\n",
    "grid_param_knn = {\n",
    "    'reg__n_neighbors': [2,3,4],\n",
    "    'reg__weights': ['uniform','distance'],\n",
    "    'reg__metric': ['euclidean'],\n",
    "    'reg__n_jobs': [-1]\n",
    "}\n",
    "\n",
    "gs_knn = (GridSearchCV(estimator=pipe_knn, \n",
    "                      param_grid=grid_param_knn, \n",
    "                      cv=2,\n",
    "                      scoring = 'neg_mean_squared_error',\n",
    "                      n_jobs = -1))\n",
    "\n",
    "\n",
    "scores_et = cross_val_score(gs_knn, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "print(scores_et)\n",
    "print('CV RMSE: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
